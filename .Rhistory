O = feature.table[comp.taxa.pos, ]
n.taxa = nrow(O)
taxa.id = rownames(O)
n.samp = ncol(O)
y = as.matrix(log(O + 1))
### 1. Initial estimates of sampling fractions and mean absulute abundances
mu = t(apply(y, 1, function(i) tapply(i, rep(1:n.grp, n.samp.grp), function(j)
mean(j, na.rm = T))))
d = colMeans(y - mu[, rep(1:n.grp, times = n.samp.grp)], na.rm = T)
## Iteration in case of missing values of y
iterNum = 0
epsilon = 100
while (epsilon > tol.EM & iterNum < max.iterNum) {
# Updating mu
mu.new = t(apply(t(t(y) - d), 1, function(i) tapply(i, rep(1:n.grp, n.samp.grp), function(j)
mean(j, na.rm = T))))
# Updating d
d.new = colMeans(y - mu.new[, rep(1:ncol(mu.new), times = n.samp.grp)], na.rm = T)
# Iteration
epsilon = sqrt(sum((mu.new - mu)^2) + sum((d.new - d)^2))
iterNum = iterNum + 1
mu = mu.new
d = d.new
}
mu.var.each = (y-t(t(mu[, rep(1:ncol(mu), times = n.samp.grp)])+d))^2
mu.var = t(apply(mu.var.each, 1, function(x) tapply(x, rep(1:n.grp, n.samp.grp), function(y)
mean(y, na.rm = T))))
sample.size = t(apply(y, 1, function(x)
unlist(tapply(x, rep(1:n.grp, n.samp.grp), function(y) length(y[!is.na(y)])))))
mu.var = mu.var/sample.size
### 2. Estimate the bias (between-group difference of sampling fractions) by E-M algorithm
bias.em.vec = rep(NA, n.grp - 1)
bias.wls.vec = rep(NA, n.grp - 1)
bias.var.vec = rep(NA, n.grp - 1)
for (i in 1:(n.grp-1)) {
Delta = mu[, 1] - mu[, 1+i]
nu = rowSums(mu.var[, c(1, 1+i)])
## 2.1 Initials
pi0_0 = 0.75
pi1_0 = 0.125
pi2_0 = 0.125
delta_0 = mean(Delta[Delta >= quantile(Delta, 0.25, na.rm = T)&
Delta <= quantile(Delta, 0.75, na.rm = T)], na.rm = T)
l1_0 = mean(Delta[Delta < quantile(Delta, 0.125, na.rm = T)], na.rm = T)
l2_0 = mean(Delta[Delta > quantile(Delta, 0.875, na.rm = T)], na.rm = T)
kappa1_0 = var(Delta[Delta < quantile(Delta, 0.125, na.rm = T)], na.rm = T)
if(is.na(kappa1_0)|kappa1_0 == 0) kappa1_0 = 1
kappa2_0 = var(Delta[Delta > quantile(Delta, 0.875, na.rm = T)], na.rm = T)
if(is.na(kappa2_0)|kappa2_0 == 0) kappa2_0 = 1
## 2.2 Apply E-M algorithm
# 2.21 Store all paras in vectors/matrices
pi0.vec = c(pi0_0); pi1.vec = c(pi1_0); pi2.vec = c(pi2_0)
delta.vec = c(delta_0); l1.vec = c(l1_0); l2.vec = c(l2_0)
kappa1.vec = c(kappa1_0); kappa2.vec = c(kappa2_0)
# 2.22 E-M iteration
iterNum = 0
epsilon = 100
while (epsilon > tol.EM & iterNum < max.iterNum) {
# print(iterNum)
## Current value of paras
pi0 = pi0.vec[length(pi0.vec)]; pi1 = pi1.vec[length(pi1.vec)]; pi2 = pi2.vec[length(pi2.vec)]
delta = delta.vec[length(delta.vec)];
l1 = l1.vec[length(l1.vec)]; l2 = l2.vec[length(l2.vec)]
kappa1 = kappa1.vec[length(kappa1.vec)]; kappa2 = kappa2.vec[length(kappa2.vec)]
## E-step
pdf0 = sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta, sqrt(nu[i])))
pdf1 = sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta + l1, sqrt(nu[i] + kappa1)))
pdf2 = sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta + l2, sqrt(nu[i] + kappa2)))
r0i = pi0*pdf0/(pi0*pdf0 + pi1*pdf1 + pi2*pdf2); r0i[is.na(r0i)] = 0
r1i = pi1*pdf1/(pi0*pdf0 + pi1*pdf1 + pi2*pdf2); r1i[is.na(r1i)] = 0
r2i = pi2*pdf2/(pi0*pdf0 + pi1*pdf1 + pi2*pdf2); r2i[is.na(r2i)] = 0
## M-step
pi0_new = mean(r0i, na.rm = T); pi1_new = mean(r1i, na.rm = T); pi2_new = mean(r2i, na.rm = T)
delta_new = sum(r0i*Delta/nu + r1i*(Delta-l1)/(nu+kappa1) + r2i*(Delta-l2)/(nu+kappa2), na.rm = T)/
sum(r0i/nu + r1i/(nu+kappa1) + r2i/(nu+kappa2), na.rm = T)
l1_new = min(sum(r1i*(Delta-delta)/(nu+kappa1), na.rm = T)/sum(r1i/(nu+kappa1), na.rm = T), 0)
l2_new = max(sum(r2i*(Delta-delta)/(nu+kappa2), na.rm = T)/sum(r2i/(nu+kappa2), na.rm = T), 0)
# Nelder-Mead simplex algorithm for kappa1 and kappa2
obj.kappa1 = function(x){
log.pdf = log(sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta+l1, sqrt(nu[i]+x))))
log.pdf[is.infinite(log.pdf)] = 0
-sum(r1i*log.pdf, na.rm = T)
}
kappa1_new = neldermead(x0 = kappa1, fn = obj.kappa1, lower = 0)$par
obj.kappa2 = function(x){
log.pdf = log(sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta+l2, sqrt(nu[i]+x))))
log.pdf[is.infinite(log.pdf)] = 0
-sum(r2i*log.pdf, na.rm = T)
}
kappa2_new = neldermead(x0 = kappa2, fn = obj.kappa2, lower = 0)$par
## Merge to the paras vectors/matrices
pi0.vec = c(pi0.vec, pi0_new); pi1.vec = c(pi1.vec, pi1_new); pi2.vec = c(pi2.vec, pi2_new)
delta.vec = c(delta.vec, delta_new)
l1.vec = c(l1.vec, l1_new); l2.vec = c(l2.vec, l2_new)
kappa1.vec = c(kappa1.vec, kappa1_new); kappa2.vec = c(kappa2.vec, kappa2_new)
## Calculate the new epsilon
epsilon = sqrt((pi0_new-pi0)^2 + (pi1_new-pi1)^2 + (pi2_new-pi2)^2 + (delta_new-delta)^2+
(l1_new-l1)^2 + (l2_new-l2)^2 + (kappa1_new-kappa1)^2 + (kappa2_new-kappa2)^2)
iterNum = iterNum+1
}
# 2.23 Estimate the bias
bias.em.vec[i] = delta.vec[length(delta.vec)]
# 2.24 The WLS estimator of bias
# Cluster 0
C0 = which(Delta >= quantile(Delta, pi1_new, na.rm = T) & Delta < quantile(Delta, 1 - pi2_new, na.rm = T))
# Cluster 1
C1 = which(Delta < quantile(Delta, pi1_new, na.rm = T))
# Cluster 2
C2 = which(Delta >= quantile(Delta, 1 - pi2_new, na.rm = T))
nu_temp = nu
nu_temp[C1] = nu_temp[C1] + kappa1_new
nu_temp[C2] = nu_temp[C2] + kappa2_new
wls.deno = sum(1 / nu_temp)
wls.nume = 1 / nu_temp
wls.nume[C0] = (wls.nume * Delta)[C0]
wls.nume[C1] = (wls.nume * (Delta - l1_new))[C1]
wls.nume[C2] = (wls.nume * (Delta - l2_new))[C2];
wls.nume = sum(wls.nume)
bias.wls.vec[i] = wls.nume / wls.deno
# 2.25 Estimate the variance of bias
bias.var.vec[i] = 1 / wls.deno
if (is.na(bias.var.vec[i])) bias.var.vec[i] = 0
}
bias.em.vec = c(0, bias.em.vec)
bias.wls.vec = c(0, bias.wls.vec)
### 3. Final estimates of mean absolute abundane and sampling fractions
mu.adj.comp = t(t(mu) + bias.em.vec)
colnames(mu.adj.comp) = grp.name; rownames(mu.adj.comp) = taxa.id
d.adj = d - rep(bias.em.vec, sapply(grp.ind, length))
names(d.adj) = sample.id
### 4. Hypothesis testing
W.numerator = matrix(apply(mu.adj.comp, 1, function(x) combn(x, 2, FUN = diff)), ncol = n.taxa)
W.numerator = t(W.numerator)
# Variance of estimated mean difference
W.denominator1 = matrix(apply(mu.var, 1, function(x) combn(x, 2, FUN = sum)), ncol = n.taxa)
# Variance of delta_hat
if (length(bias.var.vec) < 2) {
W.denominator2 = bias.var.vec
}else {
W.denominator2 = c(bias.var.vec, combn(bias.var.vec, 2, FUN = sum))
}
W.denominator = W.denominator1 + W.denominator2 + 2 * sqrt(W.denominator1 * W.denominator2)
W.denominator = t(sqrt(W.denominator))
grp.pair = combn(n.grp, 2)
colnames(W.numerator) = sapply(1:ncol(grp.pair), function(x)
paste0("mean.difference (", grp.name[grp.pair[2, x]], " - ", grp.name[grp.pair[1, x]], ")"))
colnames(W.denominator) = sapply(1:ncol(grp.pair), function(x)
paste0("se (", grp.name[grp.pair[2, x]], " - ", grp.name[grp.pair[1, x]], ")"))
rownames(W.numerator) = taxa.id; rownames(W.denominator) = taxa.id
if (length(grp.name) == 2) {
## Two-group comparison
W = W.numerator/W.denominator
p.val = sapply(W, function(x) 2*pnorm(abs(x), mean = 0, sd = 1, lower.tail = F))
q.val = p.adjust(p.val, method = adj.method)
q.val[is.na(q.val)] = 1
} else {
## Multi-group comparison: Permutation test
# Test statistics
W.each = W.numerator/W.denominator
W.each[is.na(W.each)] = 0 # Replace missing values with 0s
W = apply(abs(W.each), 1, max)
# Test statistics under null
W.null.list = lapply(1:perNum, function(x) {
set.seed(x)
mu.adj.comp.null = matrix(rnorm(n.taxa * n.grp), nrow = n.taxa, ncol = n.grp) * sqrt(mu.var)
W.numerator.null = matrix(apply(mu.adj.comp.null, 1, function(x) combn(x, 2, FUN = diff)), ncol = n.taxa)
W.numerator.null = t(W.numerator.null)
W.each.null = W.numerator.null/W.denominator
W.each.null[is.na(W.each.null)] = 0
W.null = apply(abs(W.each.null), 1, max)
return(W.null)
})
W.null = Reduce('cbind', W.null.list)
# Test results
p.val = apply(W.null - W, 1, function(x) sum(x > 0)/perNum)
q.val = p.adjust(p.val, method = adj.method)
q.val[is.na(q.val)] = 1
}
W = matrix(W, ncol = 1)
colnames(W) = "W"
res.comp = data.frame(W.numerator, W.denominator, W = W, p.val, q.val, check.names = FALSE)
### 5. Combine results from structural zeros
mu.adj = matrix(NA, nrow = n.taxa.raw, ncol = n.grp)
colnames(mu.adj) = grp.name; rownames(mu.adj) = taxa.id.raw
mu.adj[comp.taxa.pos, ] = mu.adj.comp
if (length(comp.taxa.pos) < n.taxa.raw) {
O.incomp = feature.table[-comp.taxa.pos, ]
ind.incomp = struc.zero[-comp.taxa.pos, rep(1:n.grp, times = n.samp.grp)]
y.incomp = log(O.incomp + 1)
d.incomp = t(t(1 - ind.incomp) * d) # Sampling fractions for entries considered to be structural zeros are set to be 0s
y.adj.incomp = y.incomp - d.incomp
mu.incomp = t(apply(y.adj.incomp, 1, function(i)
tapply(i, rep(1:n.grp, n.samp.grp), function(j) mean(j, na.rm = T))))
# In case of negative values for mean absolute abundances
mu.adj.incomp = mu.incomp
mu.adj.incomp[mu.adj.incomp == 0] = NA
mu.adj.incomp = t(t(mu.adj.incomp) + abs(apply(mu.incomp, 2, min)))
mu.adj.incomp[is.na(mu.adj.incomp)] = 0
}else{
mu.adj.incomp = NA
}
mu.adj[-comp.taxa.pos, ] = mu.adj.incomp
colnames(mu.adj) = paste0("mean.absolute.abundance (", grp.name, ")")
rownames(mu.adj) = taxa.id.raw
### 6. Outputs
W.numerator = matrix(apply(mu.adj, 1, function(x) combn(x, 2, FUN = diff)), ncol = n.taxa.raw)
W.numerator = t(W.numerator)
W.denominator = matrix(0, ncol = ncol(W.numerator), nrow = nrow(W.numerator))
res = data.frame(W.numerator, W.denominator, W = Inf, p.val = 0, q.val = 0, check.names = FALSE)
res[comp.taxa.pos, ] = res.comp
colnames(res) = colnames(res.comp); rownames(res) = taxa.id.raw
res = res%>%mutate(diff.abn = ifelse(q.val < alpha, TRUE, FALSE))
out = list(feature.table = feature.table, res = res, d = d.adj, mu = mu.adj, bias.em = bias.em.vec, bias.wls = bias.wls.vec)
return(out)
}
feature.table = otu_absolute; sample.var = "sample"; group.var = "nationality";
zero.cut = 0.90; lib.cut = 1000; neg.lb = TRUE
pre.process = feature_table_pre_process(feature.table, meta_data, sample.var,
group.var, zero.cut, lib.cut, neg.lb)
feature.table
grp.name = group.name; grp.ind = group.ind; adj.method = "bonferroni"
tol.EM = 1e-5; max.iterNum = 100; perNum = 1000; alpha = 0.05
feature.table = pre.process$feature.table
group.name = pre.process$group.name
group.ind = pre.process$group.ind
struc.zero = pre.process$structure.zeros
grp.name = group.name; grp.ind = group.ind; adj.method = "bonferroni"
tol.EM = 1e-5; max.iterNum = 100; perNum = 1000; alpha = 0.05
out = ANCOM_BC(feature.table, grp.name, grp.ind, struc.zero,
adj.method, tol.EM, max.iterNum, perNum, alpha)
out
out$d
runif(2, 1, 10)
runif(5, 1, 10)
raw_count <- read.table("../raw_data/rwa_data.txt", row.names = 1)
raw_count <- read.table("../raw_data/rwa_data.txt", row.names = 1)
genus <- read.table("../raw_data/bjt2d.genus.pro.profile", header = T,
row.names = 1, sep = "\t")
phe <- read.csv("../raw_data/bjt2d.config.csv", header = T,
row.names = 1, sep = "\t")
head(phe)
phe <- read.csv("../raw_data/bjt2d.config.csv", header = T,
row.names = 1)
head(phe)
acar <- phe[phe$group == 1 & phe$Time != "w12", ]
acar
genus_acar <- genus[, colnames(acar)]
dim(genus)
head(genus)
head(phe)
genus <- read.table("../raw_data/bjt2d.genus.pro.profile", header = T,
row.names = 1, sep = "\t", check.names = F)
genus_acar <- genus[, colnames(acar)]
colnames(genus)
genus_acar <- genus[, rownames(acar)]
head(raw_count)
raw_acar <- raw_count[rownames(acar), ,drop=F]
raw_acar
head(genus[,1:3])
head(genus[,1:3])*c(10, 10, 20)
t(head(genus[,1:3]))*c(10, 10, 20)
genus_acart <- t(t(genus_acar)*raw_acar[,1])
genus_acart <- round(t(t(genus_acar)*raw_acar[,1]))
head(genus_acart)[,1:3]
group.ind
group.name
raw_acar$V2
acar$Time
acar$Time=="base"
which(acar$Time=="base")
grp.ind = list(which(acar$Time=="base"), which(acar$Time=="w24"));
grp.ind
head(feature.table)[,1:4]
struc.zero
feature.table = genus_acart;
meta_data <- acar
meta_data$sample <- rownames(acar)
sample.var = "sample";
group.var = "Time";
zero.cut = 0.90;
lib.cut = 1000; neg.lb = TRUE
pre.process = feature_table_pre_process(feature.table, meta_data, sample.var,
group.var, zero.cut, lib.cut, neg.lb)
grp.name = pre.process$group.name
grp.ind = pre.process$group.ind
adj.method = "bonferroni"
tol.EM = 1e-5; max.iterNum = 100; perNum = 1000; alpha = 0.05
struc.zero <- pre.process$structure.zeros
out = ANCOM_BC(pre.process$feature.table, grp.name, grp.ind, struc.zero,
adj.method, tol.EM, max.iterNum, perNum, alpha)
View(out$res)
res = cbind(taxon = rownames(out$feature.table), out$res)
View(res)
library(rmeta)
kwmeta()
kwmeta
kwres <- kwmeta(pr = t(genus_acar), config = acar[,"Time",drop=F])
config <- acar[, "Time", drop=F]
config$Time <- droplevels(config$Time)
kwres <- kwmeta(pr = t(genus_acar), config = acar[,"Time",drop=F])
kwres <- kwmeta(pr = t(genus_acar), config = config)
head(kwres)
write.csv(res, "ancombc_two_group.csv", quote =F)
write.csv(kwres, "kw.csv", quote = F)
pr <- genus_acar[out$feature.table, ]
pr <- genus_acar[rownames(out$feature.table), ]
kwres <- kwmeta(pr = t(pr), config = config)
write.csv(kwres, "kw.csv", quote = F)
ANCOM_BC
dat <- read.csv("jdphenotype1.0917.csv", header = T, row.names = 2)[,-1]
# replace NA value
varlist <- c("highschool", "birthmethod_15", "DM_F", "cvd_f", "can_f", "smk_15",
"drk_15", "bloodtype_15")
for(i in varlist){
dat[,i] <- ifelse(dat[ ,i] ==9 | dat[ ,i]==99, NA, dat[ ,i])
}
# split the phenotype
splitD <- check_conituous(dat = dat, num = 20)
check_outlier <- function(v, coef = 3){
v[is.na(v)] <- median(v, na.rm = T)
quantiles <- quantile(v,probs=c(0.25,0.75),na.rm = T)
IQR <- quantiles[2]-quantiles[1]
res <- v < (quantiles[1]-coef*IQR)|v > (quantiles[2]+coef*IQR)
return(res)
}
check_conituous <- function(dat, num){
index <- apply(dat, 2,
function(x){length(levels(as.factor(x))) > num})
#con_index <- colnames(dat)[index]
out <- list(dat[, index], dat[, -index])
names(out) <- c("continuous", "category")
return(out)
}
boxplot_outlier_name <- function(x, sname){
check_outlier(x) -> index
name <- paste0(sname[index],"_",round(x[index],2))
res <- paste0(name, collapse = " ")
return(res)
}
boxplot_outlier <- function(dat, varname, size = 1,...){
qdat <- dat[,  varname, drop = F]
outlier <- check_outlier(qdat[, 1], ...)
qdat$label <- ifelse(outlier, rownames(qdat), "")
qdat$var <- rep(varname, nrow(qdat))
colnames(qdat)[1] <- "value"
p <- ggplot(qdat, aes(var, value))+
geom_boxplot()+
geom_text(aes(label=label), size = size, hjust=-0.1)+xlab("")+ylab("")+
theme_classic()
return(p)
}
mutivariab.out<-function(dat){
dat <- scale(dat)
Sam_num <- nrow(dat)
row_name <- rownames(dat)
if(Sam_num <= 100){
outlier.scores <- lof(dat,k=c( 5:10 ) )
out <- apply(outlier.scores,1,mean)
# out1 <- boxplot.stats(out)$out
index <- check_outlier(out)
#index <- which(out  %in% out1)
Sam_name <- row_name[index]
res <- data.frame(Sam_name,out[index])
colnames(res)<-c("Sample","lof.value")
}else{
outlier.scores <- lof(dat, k=c(floor(0.05*Sam_num):floor(0.1*Sam_num)))
out <- apply(outlier.scores,1,mean)
index <- check_outlier(out)
# out1 <- boxplot.stats(out)$out
#index <- which(out %in% out1)
Sam_name<-row_name[index]
res<-data.frame(Sam_name,out[index])
colnames(res)<-c("Sample","lof.value")
}
return(res)
}
mypcoa <- function(data, config, scale=F, method = "bray"){
if(scale==F){
dis <- vegdist(data, method = method)
pco <- pcoa(dis)
}else{
data <- scale(data, center=F, scale=T)
dis <- vegdist(data, method = method, na.rm = T)
pco <- pcoa(dis)
}
eig <- pco$value[,1]
pc1 <- eig[1]/sum(eig)*100
pc2 <- eig[2]/sum(eig)*100
pc1 <- paste0("pcoa1(",round(pc1,2),"%)")
pc2 <- paste0("pcoa2(",round(pc2,2),"%)")
dat2 <- data.frame(pco$vector[,1:2])
colnames(dat2) <- c("PC1", "PC2")
dat2$group <- as.factor(config)
plot <- ggplot(dat2,aes(PC1,PC2,color=group))+geom_point()+theme_bw()+ggtitle("pcoa")+theme(plot.title = element_text(hjust = 0.5,
size = 14),
axis.title = element_text(size = 10),
axis.text = element_text(size = 13),
panel.grid = element_blank(),
legend.title = element_text(size = 15),
legend.text = element_text(size = 13))+xlab(pc1)+ylab(pc2)
return(list(plot,dat2))
}
dat <- read.csv("jdphenotype1.0917.csv", header = T, row.names = 2)[,-1]
# replace NA value
varlist <- c("highschool", "birthmethod_15", "DM_F", "cvd_f", "can_f", "smk_15",
"drk_15", "bloodtype_15")
for(i in varlist){
dat[,i] <- ifelse(dat[ ,i] ==9 | dat[ ,i]==99, NA, dat[ ,i])
}
# split the phenotype
splitD <- check_conituous(dat = dat, num = 20)
which(apply(splitD$continuous, 1, function(x){any(is.na(x))}))
which(apply(splitD$continuous, 1, function(x){!any(is.na(x))}))
which(apply(splitD$continuous, 1, function(x){!any(is.na(x))})) -> tmp
length(tmp)
library(lof)
library(Rlof)
splitD$continuous[1:10,] -> test
test
mutivariab.out(tmp)
tmp["112900023", ]
tmp[6, ]
test[6, ]
as.numeric(test[6, ])
as.numeric(test["112900095", ])
test["112900095", ]
rownames(tmp)
rownames(test)
tmp
mutivariab.out(test)
splitD$continuous[1:20,] -> test
mutivariab.out(test)
data(iris)
df<-iris[-5]
df.lof<-lof(df,c(5:10),cores=2)
df
df[1,1] <- NA
df.lof
df[1,1] <- NA
df.lof<-lof(df,c(5:10),cores=2)
df.lof
lof(test, k=c(5:10))
?lof
nrow(splitD$continuous)
Sam_num <- 495
Sam_num <- 4952
c(floor(0.05*Sam_num):floor(0.1*Sam_num))
outlier.scores <- lof(splitD$continuous, k = c(5:20))
dim(outlier.scores)
head(outlier.scores)
mutivariab.out <- function(dat, ...){
dat <- scale(dat)
Sam_num <- nrow(dat)
row_name <- rownames(dat)
outlier.scores <- lof(dat, k = c(5:20)) # here k need to have a deep think
out <- as.data.frame(apply(outlier.scores, 1, mean))
rownames(out) <- row_name
colnames(out) <- "lof.value"
p <- boxplot_outlier(out, varname = "lof.value", ...)
# outlier
index <- check_outlier(out)
Sam_name <- row_name[index]
res <- data.frame(Sam_name,out[index])
colnames(res)<-c("Sample","lof.value")
return(list(p, res))
}
multi_outlier <- mutivariab.out(splitD$continuous, size = 3, coef = 3 )
multi_outlier <- mutivariab.out(splitD$continuous, size = 3, coef = 3 )
library(ggplot2)
multi_outlier <- mutivariab.out(splitD$continuous, size = 3, coef = 3 )
dat <- splitD$continuous
dat <- scale(dat)
Sam_num <- nrow(dat)
row_name <- rownames(dat)
outlier.scores <- lof(dat, k = c(5:20)) # here k need to have a deep think
out <- as.data.frame(apply(outlier.scores, 1, mean))
rownames(out) <- row_name
colnames(out) <- "lof.value"
p <- boxplot_outlier(out, varname = "lof.value", ...)
p <- boxplot_outlier(out, varname = "lof.value", size = 3, coef =2 )
p
index <- check_outlier(out$lof.value)
Sam_name <- row_name[index]
res <- data.frame(Sam_name,out[index])
res <- data.frame(Sam_name, out[index,1])
res
colnames(res)<-c("Sample","lof.value")
mutivariab.out <- function(dat, ...){
dat <- scale(dat)
Sam_num <- nrow(dat)
row_name <- rownames(dat)
outlier.scores <- lof(dat, k = c(5:20)) # here k need to have a deep think
out <- as.data.frame(apply(outlier.scores, 1, mean))
rownames(out) <- row_name
colnames(out) <- "lof.value"
p <- boxplot_outlier(out, varname = "lof.value", ...)
# outlier
index <- check_outlier(out$lof.value)
Sam_name <- row_name[index]
res <- data.frame(Sam_name, out[index,1])
colnames(res)<-c("Sample","lof.value")
return(list(p, res))
}
multi_outlier <- mutivariab.out(splitD$continuous, size = 3, coef = 3 )
ggsave(filename = "all.outlier.multivariable.pdf", plot = multi_outlier[[1]], width = 5, height = 4)
write.csv(multi_outlier[[2]], "multi.lof.outlier.csv")
splitD$continuous["112900551", ]
iris <- data(iris)
iris
data("iris")
iris
qdat <- iris
